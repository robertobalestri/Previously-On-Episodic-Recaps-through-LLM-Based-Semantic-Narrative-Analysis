[processing]
# Plot processing configuration
pronoun_replacement_batch_size = 40
pronoun_replacement_context_size = 40
text_simplification_batch_size = 40
semantic_correction_batch_size = 20

[face_processing]
# Face detection and processing configuration
detector = retinaface
min_confidence = 0.99
min_face_area_ratio = 0.02

# Blur detection configuration
# Method: 'laplacian' (legacy), 'gradient' (recommended), 'tenengrad', 'composite'
# GRADIENT: Less biased across skin tones, more robust for diverse faces
# TENENGRAD: Good for facial features, less affected by skin tone variations  
# COMPOSITE: Weighted combination of methods for best overall results
blur_detection_method = gradient
blur_threshold = 4.0

# Method-specific thresholds (higher = require sharper images)
blur_threshold_gradient = 5.0
blur_threshold_tenengrad = 500.0
blur_threshold_composite = 25.0

embedding_model = Facenet512
embedding_dimension = 512

# Advanced face validation parameters
enable_eye_validation = true
eye_alignment_threshold = 50.0
eye_distance_threshold = 30.0

[audio]
# Audio processing settings for WhisperX transcription and alignment
# WhisperX model for transcription (tiny, base, small, medium, large, large-v2, large-v3)
model = large-v3
# Confidence threshold for audio-based speaker assignments (0-1)
confidence_threshold = 0.8
# Minimum speaker duration in seconds
min_speaker_duration = 0.5
# HuggingFace authentication token for WhisperX models
# Get token from: https://huggingface.co/settings/tokens
# Accept model license at: https://huggingface.co/pyannote/speaker-diarization-3.1
auth_token = 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

# WhisperX transcription settings
# Device to use for transcription (cuda, cpu)
device = cuda
# Batch size for transcription (reduce if low on GPU memory)
batch_size = 16
# Compute type for CUDA (float16, float32, int8)
# Use int8 if low on GPU memory (may reduce accuracy)
compute_type = float16
# Language code for transcription (en, es, fr, etc.)
language = en
# Whether to enable speaker diarization
enable_speaker_diarization = true
# Whether to return character-level alignments (slower but more precise)
return_char_alignments = false
# Whether to enable debug output for transcription
enable_debug_output = false

[diarization]
# Pyannote speaker diarization parameters
# Minimum number of speakers to detect (set to 0 for auto-detect)
min_speakers = 1
# Maximum number of speakers to detect (set to 0 for auto-detect)
max_speakers = 50
# Minimum duration for a speaker turn in seconds (lower = more sensitive)
min_duration_on = 0.1
# Minimum duration of silence between speakers in seconds (lower = more sensitive)
min_duration_off = 0.05
# Onset threshold for speaker change detection (0.0-1.0, lower = more sensitive)
onset = 0.02
# Offset threshold for speaker change detection (0.0-1.0, lower = more sensitive)
offset = 0.05
# Minimum duration for any segment in seconds
min_duration = 0.1
# Clustering threshold (0.0-1.0, lower = more sensitive to different speakers)
threshold = 0.6

[demucs]
# Demucs vocal extraction configuration
# Enable vocal extraction for improved diarization
enable_vocal_extraction = true
# Demucs model to use (htdemucs, htdemucs_ft, mdx, mdx_extra, etc.)
model = htdemucs
# Device to use (cuda, cpu)
device = cuda
# Segment length in seconds (max 7.8 for htdemucs model)
segment_length = 7
# Overlap between segments (0.0-1.0)
overlap = 0.25
# Number of shifts for ensemble (higher = slower, better quality)
shifts = 1
# Output format (wav, mp3)
output_format = wav
# MP3 bitrate if output_format is mp3
mp3_bitrate = 320

[character_mapping]
# Character mapping settings for speaker identification
# Enable cross-episode character consistency
cross_episode_consistency = true
# Use LLM for character mapping when needed
use_llm_for_mapping = true
# Confidence threshold for character mapping (0-1)
confidence_threshold = 0.7

[speaker_identification]
# Speaker identification pipeline configuration
# Pipeline mode: audio_only, face_only, complete
# audio_only = Pure audio-based speaker identification using Pyannote
# face_only = Pure face-based speaker identification using clustering
# complete = Hybrid approach combining audio + face + character mapping
mode = audio_only
audio_enabled = true
face_enabled = true
character_mapping_enabled = true

# Face similarity threshold for speaker identification
# LLM now returns boolean confidence (is_llm_confident) instead of numeric threshold
# TRUE = LLM is confident about speaker assignment, trusted directly
# FALSE = LLM is uncertain, will be processed with face recognition
# This replaces the previous numeric threshold approach for better clarity
face_similarity_threshold = 0.8

# Similarity threshold for speaker assignment from character median comparison (0-1)
# HIGHER = Only assign speakers when face similarity to character median is very high, fewer assignments but more accurate
# LOWER = Allow speaker assignments with lower similarity to character median, more assignments but potentially less accurate
# This threshold is applied when faces are compared directly to character median embeddings
similarity_to_character_median_threshold_for_speaker_assignment = 0.5

[clustering]
# Face clustering configuration
# Cosine similarity threshold for grouping faces into clusters (0-1)
# HIGHER = Stricter clustering, faces must be very similar to group together, more clusters
# LOWER = Looser clustering, faces can be less similar and still group together, fewer clusters
cosine_similarity_threshold = 0.8

# Minimum number of faces required in a cluster to be considered valid
# HIGHER = Only large clusters kept, removes small/noisy clusters, fewer total clusters
# LOWER = Allows smaller clusters, keeps more clusters but may include noise
min_cluster_size_final = 2

# Similarity threshold for merging cluster centroids during refinement (0-1)
# HIGHER = Stricter merging, only very similar cluster centroids merged, more final clusters
# LOWER = Looser merging, merge more cluster centroids together, fewer final clusters
centroid_merge_threshold = 0.60

[multiface_processing]
# Multi-face processing configuration
enable_multiface_processing = true

# Maximum number of faces to process per dialogue line
# HIGHER = Consider more faces per dialogue, better multi-person scene handling, slower processing
# LOWER = Fewer faces per dialogue, faster processing, may miss speakers in crowded scenes
max_faces_per_dialogue = 6

# Whether to distribute probability equally among all detected faces
equal_probability_distribution = true

# Similarity threshold for LLM disambiguation when multiple face candidates exist (0-1)
# HIGHER = Only call LLM when face similarities are very close, less LLM usage, may miss disambiguation
# LOWER = Call LLM even when face similarities differ more, more LLM usage, better disambiguation
llm_disambiguation_threshold = 0.8

# Minimum total occurrences needed for a cluster to be assigned to a character
# HIGHER = Need more evidence to assign cluster to character, fewer but more confident assignments
# LOWER = Less evidence needed, more cluster assignments but potentially less reliable
cluster_minimum_occurrences = 1.5

[cluster_assignment]
# Cluster assignment enhancement configuration
enable_parity_detection = true
enable_spatial_outlier_removal = true

# Distance threshold for detecting spatial outliers (0-1, typically 0.2-0.5)
# HIGHER = More permissive, fewer clusters marked as outliers, keeps more clusters
# LOWER = Stricter outlier detection, more clusters removed as outliers, cleaner results
spatial_outlier_threshold = 0.35

# Minimum number of clusters needed before spatial outlier detection is applied
# HIGHER = Need more clusters before outlier detection runs, less outlier removal
# LOWER = Apply outlier detection with fewer clusters, more aggressive outlier removal
min_clusters_for_outlier_detection = 3

# Protection threshold for high-quality clusters from spatial outlier detection
# HIGHER = More protection for clusters with high character percentage, fewer outliers marked
# LOWER = Less protection, more clusters can be marked as outliers
# REALISTIC: Main characters in TV shows often appear in group scenes (60-80% purity is normal)
cluster_protection_percentage_threshold = 60.0

# Show all detected faces in enhanced SRT (including low-confidence ones)
# HIGHER = More transparency, shows weak face detections, more cluttered output
# LOWER = Less clutter, only shows qualified face candidates, less transparency
show_all_face_candidates_in_srt = true

# Minimum face count for cluster protection from spatial outlier detection
# HIGHER = Only protect very large clusters, more outliers marked
# LOWER = Protect smaller clusters too, fewer outliers marked
# BALANCED: Protect significant clusters while allowing small noise clusters to be removed
cluster_protection_min_faces = 1

enable_ambiguous_resolution = true

# Similarity threshold for resolving ambiguous cluster assignments using character medians (0-1)
# HIGHER = Only resolve ambiguities when character median similarity is very high, fewer resolutions
# LOWER = Resolve ambiguities with lower character median similarity, more aggressive resolution
ambiguous_resolution_threshold = 0.75

# Outlier cluster detection for wrong character assignments
enable_outlier_cluster_detection = true

# Distance threshold for detecting outlier clusters (0-1, typically 0.25-0.4)  
# HIGHER = More permissive, fewer clusters flagged as outliers
# LOWER = Stricter outlier detection, more clusters flagged as wrong assignments
# UPDATED: Increased to 0.6 to be more permissive and keep more clusters valid
outlier_distance_threshold = 0.60

# Minimum outlier score to flag a cluster as wrong assignment (1-10)
# HIGHER = Only flag clusters with very high outlier evidence, fewer false positives
# LOWER = Flag clusters with moderate outlier evidence, more sensitive detection
outlier_score_threshold = 4

[cross_episode]
# Cross-episode character matching configuration
character_similarity_threshold = 0.75

[character_median_matching]
# Character median face comparison thresholds
# Qualification threshold: minimum similarity for a face to be considered as a candidate (0-1)
# HIGHER = Stricter qualification, fewer faces considered as candidates
# LOWER = More permissive, more faces qualify for character assignment
character_median_similarity_threshold = 0.55

# Assignment threshold: minimum similarity for final character assignment (0-1)  
# HIGHER = Only assign with very high confidence, fewer assignments but more accurate
# LOWER = Allow assignments with moderate confidence, more assignments but potentially less accurate
character_median_assignment_threshold = 0.70

[sex_validation]
# Sex-based cluster validation configuration
# Enable/disable sex-based validation of speaker clusters
enable_sex_validation = true

# Maximum number of faces to analyze per cluster for sex detection (performance optimization)
# HIGHER = More accurate sex detection but slower processing
# LOWER = Faster processing but potentially less accurate
max_faces_for_sex_analysis = 10

# Minimum confidence difference between M/F required for reliable sex determination (0-100)
# HIGHER = More conservative, only use very confident sex detections, more clusters invalidated
# LOWER = Less conservative, accept lower confidence sex detections, fewer clusters invalidated
sex_confidence_threshold = 90.0

# Similarity threshold for cluster reassignment to same-sex characters (0-1)
# HIGHER = Only reassign clusters with very high similarity to character centroids
# LOWER = Allow reassignment with moderate similarity, more aggressive reassignment
sex_reassignment_similarity_threshold = 0.7

# Enable logging of all sex validation decisions for debugging
enable_sex_validation_logging = true

[output]
# Output file generation settings
enable_debug_files = false

[paths]
data_dir = data
narrative_storage_dir = narrative_storage

[api]
host = 0.0.0.0
port = 8000

[logging]
level = INFO
log_to_file = true
log_file = processing.log

# Clustering threshold (0.0-1.0, lower = more sensitive to different speakers)
threshold = 0.3
